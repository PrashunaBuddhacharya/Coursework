{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMRvInM9K4gvV6B0BHdp1NF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PrashunaBuddhacharya/Coursework/blob/main/AppliedML_TextClassification_Prashuna.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Installation**"
      ],
      "metadata": {
        "id": "oXmKiB55tCCE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFq__V2rCbKN"
      },
      "outputs": [],
      "source": [
        "!pip install datasets transformers scikit-learn pandas numpy matplotlib seaborn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Load** **Data**"
      ],
      "metadata": {
        "id": "9y53ymfUs2Eg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "print(\"Loading dataset...\")\n",
        "raw_dataset = load_dataset(\"Kenneth12/productreviewsentiment\")\n",
        "print(\"Dataset loaded successfully!\")\n",
        "print(raw_dataset)"
      ],
      "metadata": {
        "id": "1rZomJiaCpSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Table to show data analysis"
      ],
      "metadata": {
        "id": "aAx5e-IItgn4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Convert the training split to a dataframe for analysis\n",
        "df_train = pd.DataFrame(dataset['train'])\n",
        "\n",
        "# Create a summary table of characteristics\n",
        "stats = pd.DataFrame({\n",
        "    'Metric': ['Total Rows', 'Unique Text Reviews', 'Missing Values (Nulls)', 'Number of Classes'],\n",
        "    'Value': [len(df_train), df_train['Text'].nunique(), df_train['Text'].isnull().sum(), df_train['Label'].nunique()]\n",
        "})\n",
        "print(stats)"
      ],
      "metadata": {
        "id": "Ja6oDc8jQwUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentiment Distribution"
      ],
      "metadata": {
        "id": "psWirtNVurZn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show the count of each sentiment category\n",
        "distribution = df_train['Label'].value_counts().sort_index()\n",
        "\n",
        "print(\"Sentiment Distribution:\")\n",
        "print(distribution)\n",
        "\n",
        "# Percentage version (often better for identifying imbalance)\n",
        "print(\"\\nPercentage Distribution:\")\n",
        "print(df_train['Label'].value_counts(normalize=True) * 100)"
      ],
      "metadata": {
        "id": "s4zMwtz-RHGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Average Word Review"
      ],
      "metadata": {
        "id": "jQNyDGvUuZ6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate word count for each review\n",
        "df_train['word_count'] = df_train['Text'].apply(lambda x: len(x.split()))\n",
        "\n",
        "print(f\"Average words per review: {df_train['word_count'].mean():.2f}\")\n",
        "print(f\"Shortest review: {df_train['word_count'].min()} words\")\n",
        "print(f\"Longest review: {df_train['word_count'].max()} words\")"
      ],
      "metadata": {
        "id": "PbIqGFxpRRRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bargraph Of Distribution"
      ],
      "metadata": {
        "id": "WOGCjihCu75w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Fix: Use 'Label' column instead of 'label'\n",
        "counts = df_train['Label'].value_counts().sort_index()\n",
        "\n",
        "# Fix: Use the actual labels from the value_counts index\n",
        "labels = counts.index.tolist()\n",
        "\n",
        "# Define a color palette suitable for the number of categories\n",
        "colors = sns.color_palette('viridis', len(labels)) # Automatically generate colors\n",
        "\n",
        "plt.figure(figsize=(10, 6)) # Adjust figure size for better readability with more labels\n",
        "plt.bar(labels, counts, color=colors, edgecolor='black')\n",
        "plt.title('Distribution of Sentiments in Training Set', fontsize=14)\n",
        "plt.xlabel('Sentiment Class', fontsize=12)\n",
        "plt.ylabel('Number of Reviews', fontsize=12)\n",
        "\n",
        "# Adding the count labels on top of the bars\n",
        "for i, count in enumerate(counts):\n",
        "    plt.text(i, count + 10, str(count), ha='center', fontweight='bold')\n",
        "\n",
        "plt.xticks(rotation=45, ha='right') # Rotate x-axis labels for readability if they overlap\n",
        "plt.tight_layout() # Adjust layout to prevent labels from overlapping\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZbNj-IjmRedu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Histogram of Review Lengths"
      ],
      "metadata": {
        "id": "fO3-R5_3vLIZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate word counts\n",
        "df_train['word_count'] = df_train['Text'].apply(lambda x: len(x.split()))\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df_train['word_count'], bins=30, kde=True, color='purple')\n",
        "plt.title('Distribution of Review Lengths', fontsize=14)\n",
        "plt.xlabel('Number of Words', fontsize=12)\n",
        "plt.ylabel('Frequency', fontsize=12)\n",
        "plt.axvline(df_train['word_count'].mean(), color='red', linestyle='--', label=f'Mean: {df_train[\"word_count\"].mean():.1f}')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oMx0LsHSSJFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing Libraries"
      ],
      "metadata": {
        "id": "tY_Ej4JoyhiW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import the basic tools for data and text\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "\n",
        "# These are the specific NLP tools from the NLTK library\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Downloading the 'rules' for words and grammar\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Setting up my cleaning tools\n",
        "stop_words_list = stopwords.words('english')\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "3gkj2TYBsnsY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing"
      ],
      "metadata": {
        "id": "fYijU9FYs0Rz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "sample_review = \"I LOVED this product! It was the best, even though it was arriving late. 10/10!!\"\n",
        "\n",
        "# Step 1: Lowercase\n",
        "step1 = sample_review.lower()\n",
        "# Step 2: Remove punctuation and numbers\n",
        "step2 = re.sub(r'[^a-z\\s]', '', step1)\n",
        "# Step 3: Split into words\n",
        "step3 = step2.split()\n",
        "# Step 4: Remove stopwords and Lemmatize\n",
        "step4 = [lemmatizer.lemmatize(word) for word in step3 if word not in stop_words_list]\n",
        "\n",
        "print(f\"Original: {sample_review}\")\n",
        "print(f\"Cleaned:  {' '.join(step4)}\")"
      ],
      "metadata": {
        "id": "a7nE4Zv6ys7t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cleaning to the Whole Dataset"
      ],
      "metadata": {
        "id": "8mw1QUaIzENR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def my_cleaning_function(raw_text):\n",
        "    # Standardizing text\n",
        "    text = raw_text.lower()\n",
        "    # Getting rid of symbols and numbers\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    # Tokenizing\n",
        "    words = text.split()\n",
        "    # Final cleanup\n",
        "    final_words = [lemmatizer.lemmatize(w) for w in words if w not in stop_words_list]\n",
        "    return \" \".join(final_words)\n",
        "\n",
        "\n",
        "df_train['cleaned_text'] = df_train['Text'].apply(my_cleaning_function)\n",
        "\n",
        "\n",
        "df_train[['Text', 'cleaned_text']].head(10)"
      ],
      "metadata": {
        "id": "QH7Omk41zIsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fixing Sentiment Label Typos"
      ],
      "metadata": {
        "id": "kX3lhNlwztwZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_sentiment_typos(label):\n",
        "    label = str(label).lower().strip()\n",
        "    if 'posit' in label: # This catches 'positive', 'positve', and 'positiiive'\n",
        "        return 2\n",
        "    elif 'negat' in label:\n",
        "        return 0\n",
        "    else:\n",
        "        return 1\n",
        "\n",
        "df_train['final_target'] = df_train['Label'].apply(fix_sentiment_typos)\n",
        "\n",
        "\n",
        "print(\"Cleaned Sentiment Counts:\")\n",
        "print(df_train['final_target'].value_counts())"
      ],
      "metadata": {
        "id": "XznaDXCuz2yl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating the TF-IDF Feature Matrix"
      ],
      "metadata": {
        "id": "pig_1UJb0Jh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Initializing the Vectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=2500)\n",
        "\n",
        "# Transforming the 'cleaned_text' into numerical data (X)\n",
        "X_train = vectorizer.fit_transform(df_train['cleaned_text'])\n",
        "\n",
        "# Our target variable (y)\n",
        "y_train = df_train['final_target']\n",
        "\n",
        "print(f\"Success! The Training Matrix has {X_train.shape[0]} rows and {X_train.shape[1]} features.\")"
      ],
      "metadata": {
        "id": "VDMVxRyd0Tvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Training (LR and SVM)\n",
        "\n"
      ],
      "metadata": {
        "id": "LUbLLASe2S7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# 1. Initialize models with 'balanced' weights to handle the imbalance we found\n",
        "lr_model = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
        "svm_model = LinearSVC(class_weight='balanced', C=1.0)\n",
        "\n",
        "# 2. Train the models\n",
        "lr_model.fit(X_train, y_train)\n",
        "svm_model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Training Complete. Models are ready for evaluation.\")"
      ],
      "metadata": {
        "id": "ymKMJq8k2SZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Performance Evaluation"
      ],
      "metadata": {
        "id": "6A_e1B-T2fOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Get predictions from both models\n",
        "lr_predictions = lr_model.predict(X_train)\n",
        "svm_predictions = svm_model.predict(X_train)\n",
        "\n",
        "# 2. Calculate simple accuracy scores\n",
        "lr_accuracy = accuracy_score(y_train, lr_predictions)\n",
        "svm_accuracy = accuracy_score(y_train, svm_predictions)\n",
        "\n",
        "print(f\"Logistic Regression Accuracy: {lr_accuracy * 100:.2f}%\")\n",
        "print(f\"Support Vector Machine Accuracy: {svm_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "WPOhLr-M3hVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Confusion Matrices: Logistic Regression Matrix"
      ],
      "metadata": {
        "id": "6zp5g_KA4dS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# Create the matrix for Logistic Regression\n",
        "cm_lr = confusion_matrix(y_train, lr_predictions)\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Neg', 'Neu', 'Pos'], yticklabels=['Neg', 'Neu', 'Pos'])\n",
        "plt.title('Confusion Matrix: Logistic Regression')\n",
        "plt.xlabel('Predicted Sentiment')\n",
        "plt.ylabel('Actual Sentiment')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JRAtA8s93oBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SVM Matrix"
      ],
      "metadata": {
        "id": "boyZL8OJ4uxY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the matrix for SVM\n",
        "cm_svm = confusion_matrix(y_train, svm_predictions)\n",
        "\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.heatmap(cm_svm, annot=True, fmt='d', cmap='Oranges',\n",
        "            xticklabels=['Neg', 'Neu', 'Pos'], yticklabels=['Neg', 'Neu', 'Pos'])\n",
        "plt.title('Confusion Matrix: Support Vector Machine')\n",
        "plt.xlabel('Predicted Sentiment')\n",
        "plt.ylabel('Actual Sentiment')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Ql57xFIU31S9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Detailed Performance Metrics"
      ],
      "metadata": {
        "id": "Q4Bj_wzZ4ymC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(\"--- Detailed Metrics for Logistic Regression ---\")\n",
        "print(classification_report(y_train, lr_predictions, target_names=['Negative', 'Neutral', 'Positive']))\n",
        "\n",
        "print(\"\\n--- Detailed Metrics for Support Vector Machine ---\")\n",
        "print(classification_report(y_train, svm_predictions, target_names=['Negative', 'Neutral', 'Positive']))"
      ],
      "metadata": {
        "id": "dzRWoBJ-33ni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparison Bar Chart"
      ],
      "metadata": {
        "id": "ollSBkHS44yD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Data from your results (replace with your actual scores)\n",
        "models = ['Logistic Regression', 'SVM']\n",
        "accuracy = [0.88, 0.92] # Example scores\n",
        "f1_scores = [0.85, 0.90] # Example scores\n",
        "\n",
        "x = np.arange(len(models))\n",
        "width = 0.35\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.bar(x - width/2, accuracy, width, label='Accuracy', color='skyblue')\n",
        "ax.bar(x + width/2, f1_scores, width, label='F1-Score', color='salmon')\n",
        "\n",
        "ax.set_ylabel('Scores')\n",
        "ax.set_title('Model Performance Comparison')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(models)\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TdNyw4U74OjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The \"Top 10 Words\" Bar Chart"
      ],
      "metadata": {
        "id": "4YV2kKpW5AFk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_top_words(model, vectorizer, category_index, title):\n",
        "    # Get word names and their 'importance' weights\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "    # For SVM, we look at the coefficients\n",
        "    coefs = model.coef_[category_index]\n",
        "\n",
        "    # Get top 10 words\n",
        "    top_indices = np.argsort(coefs)[-10:]\n",
        "    top_words = [feature_names[i] for i in top_indices]\n",
        "    top_weights = [coefs[i] for i in top_indices]\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.barh(top_words, top_weights, color='lightgreen')\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Importance Weight')\n",
        "    plt.show()\n",
        "\n",
        "# Show top 10 words for 'Positive' class (Index 2)\n",
        "plot_top_words(svm_model, vectorizer, 2, \"Top 10 Words Indicating Positive Sentiment (SVM)\")"
      ],
      "metadata": {
        "id": "goxAWCVk4RRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Multi-Class ROC Curve"
      ],
      "metadata": {
        "id": "DvL81J3V5E2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# Binarize the output for 3 classes\n",
        "y_test_bin = label_binarize(y_train, classes=[0, 1, 2])\n",
        "# Logistic Regression provides probabilities needed for ROC\n",
        "y_score = lr_model.predict_proba(X_train)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "colors = ['red', 'blue', 'green']\n",
        "labels = ['Negative', 'Neutral', 'Positive']\n",
        "\n",
        "for i in range(3):\n",
        "    fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    plt.plot(fpr, tpr, color=colors[i], label=f'ROC {labels[i]} (area = {roc_auc:.2f})')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Multi-Class ROC Curve (Logistic Regression)')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Obe7gPPS4Wnb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}